
# LLM-Agents-TFM

Trabajo de Fin de M√°ster: **Agentes basados en LLM con arquitectura RAG** para generaci√≥n y correcci√≥n de ex√°menes.  
El proyecto usa [Ollama](https://ollama.com/) junto con `llama-index`, `chromadb` y scripts en Python.

---

## üöÄ Requisitos

- **Python 3.10+**
- Instalar dependencias:

  ```bash
  pip install -r requirements.txt
  ```

(requisitos principales: `pandas`, `numpy`, `requests`, `llama-index`, `chromadb`, `rapidfuzz`, `openpyxl`)

* **Ollama instalado** y modelos descargados:

  ```bash
  ollama pull llama3
  ollama pull llama3:8b
  ollama pull mistral
  ollama pull deepseek-r1
  ollama pull nomic-embed-text
  ollama pull all-minilm
  ollama pull mxbai-embed-large
  ```

---

## üê≥ Ejecuci√≥n con Docker

Incluye `docker-compose.yml` para levantar el entorno con soporte GPU:

```bash
docker compose up -d
```

Para pararlo:

```bash
docker compose down
```

üëâ Si no tienes GPU, elimina la secci√≥n `deploy.resources.devices` del `docker-compose.yml`.

---

## üìÇ Dataset: Simpsons Multilingual

Corpus de prueba en tres idiomas:

* `simpsons_data_es`: 13 episodios en **espa√±ol**
* `simpsons_data_en`: 13 episodios en **ingl√©s**
* `simpsons_data_ca`: 13 episodios en **catal√°n**

Formato de cada archivo:

* **ES**: T√≠tulo, Temporada, Episodio, Sinopsis, Personajes destacados
* **EN**: Title, Season, Episode, Synopsis, Main characters
* **CA**: T√≠tol, Temporada, Episodi, Sinopsi, Personatges destacats

**Sugerencias**:

* Indexar cada idioma en colecciones distintas de ChromaDB para comparar *recall/precision*.
* Usar archivos homog√©neos (ej. `s01eXX.txt`) para an√°lisis cruzado.

---

## ‚öôÔ∏è Scripts principales

### 1. `simpsons_script.py`

Genera el dataset en la carpeta `simpsons_data` (solo ES, el resto fue traducido con ChatGPT). Los documentos quedan en `/data`.

```bash
python simpsons_script.py
```

---

### 2. `build_index.py`

Crea la base de datos e √≠ndice en **ChromaDB** a partir del dataset.
Es un archivo de prueba: en `benchmark_rag_tokenized.py` se reutiliza esta funcionalidad dentro del propio c√≥digo.

```bash
python build_index.py --data_dir simpsons_data_es --collection simpsons_es
```

---

### 3. `benchmark_rag_tokenized.py`

Ejecuta benchmarks de RAG con recuperaci√≥n + generaci√≥n.
Mide latencias, tokens y m√©tricas de groundedness.

Ejemplo de uso:

```bash
python benchmark_rag_tokenized.py \
  --data_dir simpsons_data_es \
  --gold gold_test_es.csv \
  --llm llama3 \
  --embed nomic-embed-text \
  --top_k 3 \
  --lang es \
  --output results_es.csv
```

**Par√°metros destacados:**

* `--lang` ‚Üí idioma (`es`, `en`, `ca`)
* `--llm` ‚Üí modelo LLM en Ollama
* `--embed` ‚Üí modelo de embeddings
* `--top_k` ‚Üí n¬∫ de chunks recuperados
* `--use_chroma` ‚Üí activa Chroma como vector store
* `--groundedness semantic|lexical` ‚Üí c√≥mo medir groundedness (*semantic* por defecto)

---

### 4. `aggregate_with_audit.py`

Unifica m√∫ltiples resultados (CSV/JSONL) en un √∫nico archivo y genera auditor√≠a.
De esta manera se asegura que los archivos contienen la cantidad de filas correcta.

```bash
python aggregate_with_audit.py \
  --input-dir bench_out \
  --output bench_out/aggregate.csv \
  --audit bench_out/_audit.csv
```

**Opcional:**

* `--schema-json` o `--schema-csv` para fijar un esquema de columnas can√≥nico.
* `--parquet` para exportar tambi√©n en formato Parquet (no se ha llegado a utilizar).

---

### 5. `analyze_schema_bench.py`

Analiza el `aggregate.csv` y produce m√©tricas textuales y cuantitativas.

```bash
python analyze_schema_bench.py \
  --in bench_out/aggregate.csv \
  --outdir reports
```

**Genera:**

* M√©tricas textuales: **EM, F1, ROUGE-L, Levenshtein**
* Latencias y eficiencia (`tokens_per_sec`)
* Correlaciones y leaderboard de mejores combinaciones (en este caso no se explot√≥ mucho)
* Excel resumen (`reports/summary.xlsx`)
* Tablas por idioma, LLM, embedding, top\_k y preguntas

---

## üìä Flujo de trabajo

1. Generar dataset ‚Üí `simpsons_script.py`
2. Indexar documentos ‚Üí `build_index.py`
3. Ejecutar benchmark ‚Üí `benchmark_rag_tokenized.py`
4. Agregar resultados ‚Üí `aggregate_with_audit.py`
5. Analizar m√©tricas ‚Üí `analyze_schema_bench.py`

---

## üìé Notas

* Los resultados incluyen m√©tricas de groundedness y soporte de recuperaci√≥n (`groundedness_pred_0_1`, `retrieval_support_gold_0_1`).
* Para m√°s fidelidad, se recomienda usar `--groundedness semantic` (requiere embeddings).
* `rapidfuzz` mejora el c√°lculo de similitud de Levenshtein.
